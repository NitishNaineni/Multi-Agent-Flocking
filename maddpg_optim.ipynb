{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import arg\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import gym\n",
    "from boids import parallel_env as penv\n",
    "from boids import config\n",
    "from ddpg import DDPG\n",
    "from args import parameter_args\n",
    "from ou_noise import ouNoise\n",
    "from replay import Prioritized_Experience_Replay as PER\n",
    "from gym.spaces import Box\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\")\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_experience(env,obs,args,agent_per,adversary_per,agent_ddpg,adversary_ddpg,agent_noise,adversary_noise):\n",
    "    count=0\n",
    "    DONE=False\n",
    "    score_agent=0\n",
    "    score_adversary=0\n",
    "    done = env.aec_env.dones\n",
    "    # print(done)\n",
    "    while not DONE:\n",
    "        # print(\"Iteration Count \",count)\n",
    "        actions={}\n",
    "        loss={}\n",
    "        for key in obs:\n",
    "            if(key.find('adversary') != -1):\n",
    "                if done[key] == False:\n",
    "                    temp=adversary_ddpg.get_actions(obs[key])\n",
    "                    # print(\"NN \", temp)\n",
    "                    temp=temp.detach().numpy() + adversary_noise.add_noise()\n",
    "                    # print(\"Noise \", temp)\n",
    "                    actions[key]=temp.astype(np.float32)\n",
    "                    # actions[key] = np.clip(actions[key], env.action_space(key).low[0], env.action_space(key).high[0])\n",
    "                else:\n",
    "                     actions[key] = None\n",
    "\n",
    "            else:\n",
    "                if done[key] == False:\n",
    "                    temp=agent_ddpg.get_actions(obs[key])\n",
    "                    temp=temp.detach().numpy() + agent_noise.add_noise()\n",
    "                    actions[key]=temp.astype(np.float32)\n",
    "                    # actions[key] = np.clip(actions[key], env.action_space(key).low[0], env.action_space(key).high[0])\n",
    "                    # actions[key] = np.array([1,0,0,0,1])\n",
    "                else: \n",
    "                    actions[key] = None\n",
    "        nex_obs, reward, done,_= env.step(actions)\n",
    "        # print(nex_obs)\n",
    "        env.render()\n",
    "        # print(actions['agent_0'])\n",
    "        loss=0\n",
    "        for key in obs:\n",
    "            if(key.find('adversary') != -1):\n",
    "                loss=adversary_ddpg.get_loss(obs[key],actions[key],nex_obs[key])\n",
    "                adversary_per.push(loss,obs[key],actions[key],reward[key],nex_obs[key],done[key])\n",
    "                score_adversary+=reward[key]\n",
    "            else:\n",
    "                loss=agent_ddpg.get_loss(obs[key],actions[key],nex_obs[key])\n",
    "                agent_per.push(loss,obs[key],actions[key],reward[key],nex_obs[key],done[key])\n",
    "                score_agent+=reward[key]\n",
    "        obs=nex_obs\n",
    "        count+=1\n",
    "        if(count>=args.timesteps or all(x==True for x in done.values())):\n",
    "            DONE=True\n",
    "    # print(\"Adversary Reward \", score_adversary)\n",
    "    # print(\"Agent Reward\",score_agent )\n",
    "    # env.close()\n",
    "    return score_agent,score_adversary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\naine\\Documents\\github\\Multi-Agent-Flocking\\maddpg_optim.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000004?line=43'>44</a>\u001b[0m obs\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000004?line=44'>45</a>\u001b[0m train_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000004?line=45'>46</a>\u001b[0m score_agent,score_adversary\u001b[39m=\u001b[39m collect_experience(env,obs,args,agent_per,adversary_per,agent_ddpg,adversary_ddpg,agent_noise,adversary_noise)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000004?line=46'>47</a>\u001b[0m agent_scores\u001b[39m.\u001b[39mappend(score_agent)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000004?line=47'>48</a>\u001b[0m adversary_scores\u001b[39m.\u001b[39mappend(score_adversary)\n",
      "\u001b[1;32mc:\\Users\\naine\\Documents\\github\\Multi-Agent-Flocking\\maddpg_optim.ipynb Cell 4'\u001b[0m in \u001b[0;36mcollect_experience\u001b[1;34m(env, obs, args, agent_per, adversary_per, agent_ddpg, adversary_ddpg, agent_noise, adversary_noise)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000003?line=14'>15</a>\u001b[0m temp\u001b[39m=\u001b[39madversary_ddpg\u001b[39m.\u001b[39mget_actions(obs[key])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000003?line=15'>16</a>\u001b[0m \u001b[39m# print(\"NN \", temp)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000003?line=16'>17</a>\u001b[0m temp\u001b[39m=\u001b[39mtemp \u001b[39m+\u001b[39;49m adversary_noise\u001b[39m.\u001b[39;49madd_noise()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000003?line=17'>18</a>\u001b[0m \u001b[39m# print(\"Noise \", temp)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Multi-Agent-Flocking/maddpg_optim.ipynb#ch0000003?line=18'>19</a>\u001b[0m actions[key]\u001b[39m=\u001b[39mtemp\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\naine\\def\\lib\\site-packages\\torch\\_tensor.py:732\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/_tensor.py?line=729'>730</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/_tensor.py?line=730'>731</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/_tensor.py?line=731'>732</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/_tensor.py?line=732'>733</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/_tensor.py?line=733'>734</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = penv(config=config)\n",
    "    obs=env.reset()\n",
    "    num_states_agent=env.observation_space('agent_0').shape[0]\n",
    "    num_states_adv=env.observation_space('adversary_0').shape[0]\n",
    "\n",
    "    num_actions=env.action_space('agent_0').shape[0]\n",
    "    args=parameter_args()\n",
    "    \n",
    "    agent_per=PER(args.buffer_size_agent,args.exp_alpha,args.batch_size)\n",
    "    adversary_per=PER(args.buffer_size_adversary,args.exp_alpha,args.batch_size)\n",
    "    \n",
    "    agent_model_path = 'agent_ddpg.pth'\n",
    "    adversary_model_path = 'adversary_ddpg.pth'\n",
    "\n",
    "    agent_ddpg=DDPG(num_states_agent,num_actions,args)\n",
    "    adversary_ddpg=DDPG(num_states_adv,num_actions,args)\n",
    "    \n",
    "    # if (os.path.isfile(agent_model_path)):\n",
    "    #     print(\"Found the Agent DDPG Model, loading that\")\n",
    "    #     agent_ddpg_temp = torch.load(agent_model_path)\n",
    "    #     agent_ddpg.actor.load_state_dict(agent_ddpg_temp.actor.state_dict())\n",
    "    #     agent_ddpg.target_actor.load_state_dict(agent_ddpg_temp.target_actor.state_dict())\n",
    "\n",
    "    \n",
    "    # if (os.path.isfile(adversary_model_path)):\n",
    "    #     print(\"Found the Adversary DDPG Model, loading that\")\n",
    "    #     adversary_ddpg_temp = torch.load(adversary_model_path)\n",
    "    #     adversary_ddpg.actor.load_state_dict(adversary_ddpg_temp.actor.state_dict())\n",
    "    #     adversary_ddpg.target_actor.load_state_dict(adversary_ddpg_temp.target_actor.state_dict())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    agent_noise=ouNoise(np.zeros(env.action_space('agent_0').shape[0]))\n",
    "    adversary_noise=ouNoise(np.zeros(env.action_space('agent_0').shape[0]))\n",
    "    agent_scores=[]\n",
    "    adversary_scores=[]\n",
    "    train_agent = True\n",
    "    train_count = 0\n",
    "    for i in range(args.epoch):\n",
    "       \n",
    "        print(\"Current Epoch \",i)\n",
    "        obs=env.reset()\n",
    "        train_count += 1\n",
    "        score_agent,score_adversary= collect_experience(env,obs,args,agent_per,adversary_per,agent_ddpg,adversary_ddpg,agent_noise,adversary_noise)\n",
    "        agent_scores.append(score_agent)\n",
    "        adversary_scores.append(score_adversary)\n",
    "        final_scores = np.add(agent_scores,  adversary_scores)\n",
    "        if train_agent and (len(agent_per.buffer)>=args.batch_size):\n",
    "            # print(\"Training Agent\")\n",
    "            agent_ddpg.policyUpdate(agent_per,args.buffer_size_agent)\n",
    "            # if i%5==0\n",
    "            agent_ddpg.saveModel(name='agent')\n",
    "        elif not train_agent and  (len(adversary_per.buffer)>=args.batch_size):\n",
    "            # print(\"Training Adversary\")\n",
    "            adversary_ddpg.policyUpdate(adversary_per,args.buffer_size_adversary)\n",
    "            adversary_ddpg.saveModel(name='adversary')\n",
    "        \n",
    "           \n",
    "        if train_count == 20:\n",
    "            train_agent = not train_agent\n",
    "            train_count = 0\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        figure, axis = plt.subplots(1,3)\n",
    "        figure.set_figheight(5)\n",
    "        figure.set_figwidth(20)\n",
    "        axis[0].plot(agent_scores)\n",
    "        axis[0].set_title(\"Agent Scores\")\n",
    "        axis[1].plot(adversary_scores)\n",
    "        axis[1].set_title(\"Adversary Scores\")\n",
    "        axis[2].plot(final_scores)\n",
    "        axis[2].set_title(\"Added Scores\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.pause(0.01)\n",
    "        \n",
    "        \n",
    "    env.close()\n",
    "    # clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5b53d600c30c432c9f427e0099044907f5b90ab104622449e78aa45f8a63a20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('def': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
